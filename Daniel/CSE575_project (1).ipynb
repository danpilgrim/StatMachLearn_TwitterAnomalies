{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "YGLdhob6HPKD",
    "outputId": "bbabedde-baf7-43b7-f1ec-2ec4dc7ed1f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in Directory\n",
      "['IRAhandle_tweets_4.csv', 'IRAhandle_tweets_5.csv', 'IRAhandle_tweets_7.csv', 'glove.twitter.27B.100d.txt', 'IRAhandle_tweets_6.csv', 'IRAhandle_tweets_2.csv', '.DS_Store', 'IRAhandle_tweets_3.csv', 'IRAhandle_tweets_1.csv', 'RussianTrollData_Preprocessing.ipynb', 'glove.twitter.27B.50d.txt', 'bigboy.csv', 'nonRTs copy 2.csv', 'CSE575_project.ipynb', 'README.md', 'glove.twitter.27B.25d.txt', 'IRAhandle_tweets_12.csv', 'IRAhandle_tweets_13.csv', 'kowalski_analysis.py', 'IRAhandle_tweets_11.csv', 'glove.twitter.27B.200d.txt', 'IRAhandle_tweets_10.csv', 'russiantrolldata_preprocessing.py', '.ipynb_checkpoints', '.git', 'RussianTrollData_Preprocessing-Copy1.ipynb', 'dashboard_x_usa_x_filter_nativeretweets.csv', 'IRAhandle_tweets_8.csv', 'nonRTs.csv', 'ira_tweets_csv_hashed.csv', 'tweetData.csv', 'IRAhandle_tweets_9.csv']\n"
     ]
    }
   ],
   "source": [
    "### General definitions -----------------------------------------------------\n",
    "### imports \n",
    "import csv\n",
    "import os\n",
    "import string\n",
    "import pandas\n",
    "import re\n",
    "import numpy\n",
    "\n",
    "\n",
    "### displays files in directory\n",
    "print(\"Files in Directory\")\n",
    "listOfFileNames = os.listdir(os.getcwd()) #finds file names in directory\n",
    "print(listOfFileNames)\n",
    "\n",
    "## Word vectors from https://nlp.stanford.edu/projects/glove/\n",
    "_50d_wordvec = \"glove.6B.50d.txt\"\n",
    "_100d_wordvec = \"glove.6B.100d.txt\"\n",
    "_300d_wordvec = \"glove.6B.300d.txt\"\n",
    "\n",
    "_twitter_50d_wordvec = \"glove.twitter.27B.50d.txt\"\n",
    "\n",
    "DIMENSIONALITY = 50\n",
    "\n",
    "## Twitter files \n",
    "### all russian bot tweets\n",
    "bigRussiaFile = \"ira_tweets_csv_hashed.csv\"    # all russian bot tweets\n",
    "smallRussiaFile = \"IRAhandle_tweets_1.csv\" #smaller sample\n",
    "\n",
    "# normal tweets\n",
    "normalTweets = \"dashboard_x_usa_x_filter_nativeretweets.csv\" # normal tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pHtnfO802AtX"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "GltixBlRWn9H",
    "outputId": "8d9b8199-0ea4-4f1e-ddd0-2f5d7710ee89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russia Tweets Info \"rtd\"\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index(['external_author_id', 'author', 'content', 'region', 'language',\n",
      "       'publish_date', 'harvested_date', 'following', 'followers', 'updates',\n",
      "       'post_type', 'account_type', 'retweet', 'account_category',\n",
      "       'new_june_2018', 'alt_external_id', 'tweet_id', 'article_url',\n",
      "       'tco1_step1', 'tco2_step1', 'tco3_step1'],\n",
      "      dtype='object')\n",
      "\n",
      "Normal Tweet Info \"ntd\"\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index(['Tweet Id', 'Date', 'Hour', 'User Name', 'Nickname', 'Bio',\n",
      "       'Tweet content', 'Favs', 'RTs', 'Latitude', 'Longitude', 'Country',\n",
      "       'Place (as appears on Bio)', 'Profile picture', 'Followers',\n",
      "       'Following', 'Listed', 'Tweet language (ISO 639-1)', 'Tweet Url'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Creates pandas dataframe objects -----------------------------------------------------\n",
    "\n",
    "print(\"Russia Tweets Info \\\"rtd\\\"\")\n",
    "rtd = pandas.read_csv(smallRussiaFile) \n",
    "print(type(rtd))                     \n",
    "rtd_TotalRows, rtd_TotalColumns = rtd.shape\n",
    "print(rtd.columns)\n",
    "\n",
    "print(\"\\nNormal Tweet Info \\\"ntd\\\"\")\n",
    "ntd = pandas.read_csv(normalTweets)\n",
    "print(type(ntd))                      \n",
    "ntd_TotalRows, ntd_TotalColumns = ntd.shape\n",
    "print(ntd.columns)\n",
    "\n",
    "#rtd.head(3)                                 # top 3 rows\n",
    "#rtd['publish_data']                         # gives pandas.series of that column\n",
    "#rtd.publish_data                            # same thing\n",
    "#rtd['author','content','tempurature']       # mutiple\n",
    "#rtd['external_author_id'].max()\n",
    "#rtd.describe()                              #describes data\n",
    "#rtd[rtd.author==\"10_GOP\"]                   # gives rows with author = 10_GOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "K6js1gzax8U6"
   },
   "outputs": [],
   "source": [
    "### Preprocessing Data ----------------------------------------------------------------------\n",
    "\n",
    "## we only want English tweets\n",
    "rtd = rtd[rtd['language']=='English']\n",
    "ntd = ntd[ntd[\"Tweet language (ISO 639-1)\"]=='en']\n",
    "\n",
    "# we only want text data\n",
    "rtd_tweettexts = rtd['content']\n",
    "ntd_tweettexts = ntd['Tweet content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "LLFHo5AdeQwA",
    "outputId": "80c48f8a-f78f-474d-cc51-86a2b1d95f63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#nlp']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "## tokenize tweet function defintion ------------------------------------------------\n",
    "import re\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=True):  # Tweet tokenizer\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(preprocess(tweet))\n",
    "print(type(preprocess(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "QNMqAAPgrkHb",
    "outputId": "56522b3a-3359-4658-f66f-ed8ca58aa24f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/danz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Word Vectors -----------------------------------------------------------------------------\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "glove_wordmap = {}  # given word from 30d data, returns vector from 30D data\n",
    "lmtzr = WordNetLemmatizer()\n",
    "for line in open(_twitter_50d_wordvec):           \n",
    "    word = line.split(' ')[0]\n",
    "    vec = line.split(' ')[1:]\n",
    "    glove_wordmap[word] = numpy.asarray(vec, dtype='float32')\n",
    "\n",
    "# given word, return glove wordvec\n",
    "def wordLookup(word):\n",
    "    vec = glove_wordmap.get(word)\n",
    "    if vec is not None:\n",
    "      return vec   # returns ndarray 50x1, dtype=float32)\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "def tokenListVector(tokenList):\n",
    "    vector_dim = len(wordLookup(\"test\"))\n",
    "    vec = numpy.zeros(vector_dim)\n",
    "    \n",
    "    for token in tokenList:\n",
    "        wordVec = wordLookup(token)\n",
    "        if wordVec is not None:\n",
    "            vec = vec + wordVec\n",
    "            \n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "amHFIeS4cT9J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['content', 'Normal', 'Bot', 'Type'], dtype='object')\n",
      "Index(['content', 'Normal', 'Bot', 'Type'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of                                                   content  Normal  Bot    Type\n",
       "0       Wind 3.2 mph NNE. Barometer 30.20 in, Rising s...       1    0  Normal\n",
       "2       Good. Morning. #morning #Saturday #diner #VT #...       1    0  Normal\n",
       "3       @gratefuldead recordstoredayus 🌹🌹🌹 @ TOMS MUSI...       1    0  Normal\n",
       "4       Egg in a muffin!!! (@ Rocket Baby Bakery - @ro...       1    0  Normal\n",
       "5       @lyricwaters should've gave the neighbor  a bu...       1    0  Normal\n",
       "6       On the way to CT! (@ Mamaroneck, NY in Mamaron...       1    0  Normal\n",
       "7       We're #hiring! Read about our latest #job open...       1    0  Normal\n",
       "8       Me... @ Montgomery Scrap Corporation https://t...       1    0  Normal\n",
       "9       BAYADA Home Health Care: Home Health Registere...       1    0  Normal\n",
       "10      Shift Supervisor Trainee - CVS Health: (#OCEAN...       1    0  Normal\n",
       "12      Although I am not endorsing a candidate, I tho...       1    0  Normal\n",
       "13      I think spring finally has arrived! (@ Milwauk...       1    0  Normal\n",
       "14      Want to work in #Burlington, MA? View our late...       1    0  Normal\n",
       "15      Want to work in #Charlotte, NC? View our lates...       1    0  Normal\n",
       "16      you'll love this @34jaIIen https://t.co/fL7YNL...       1    0  Normal\n",
       "18      Can you recommend anyone for this #job? Clinic...       1    0  Normal\n",
       "19      I'm at @InNOutBurger in Long Beach, CA https:/...       1    0  Normal\n",
       "20      Don’t think about what can happen in a month. ...       1    0  Normal\n",
       "21      Vacation here I come !! @ Official JetBlue Ter...       1    0  Normal\n",
       "23      Can you recommend anyone for this #Hospitality...       1    0  Normal\n",
       "24      We're #hiring! Click to apply: Manager Trainee...       1    0  Normal\n",
       "25                    I'm at Home https://t.co/fDCP5atErT       1    0  Normal\n",
       "26      See our latest #Savannah, GA #job and click to...       1    0  Normal\n",
       "27      Want to work at Macy's Beauty? We're #hiring i...       1    0  Normal\n",
       "28      Can you recommend anyone for this #job? Regist...       1    0  Normal\n",
       "29      🍩 is 🔑 @ Jupiter Donuts North Palm https://t.c...       1    0  Normal\n",
       "30      in #TomsRiver, NJ: Hourly Home Care Nurses Nee...       1    0  Normal\n",
       "31      Want to work at Aerotek? We're #hiring in #Dul...       1    0  Normal\n",
       "32      It is a beautiful Saturday morning!!! Get out ...       1    0  Normal\n",
       "33      Remember that time we got Lucas some #DoleWhip...       1    0  Normal\n",
       "...                                                   ...     ...  ...     ...\n",
       "243861  .@fadifawaz \"happy\" following George Michael's...       0    1     Bot\n",
       "243862  Kristen Stewart shaves her head in drastic ima...       0    1     Bot\n",
       "243863  Anti-abortion March for Life should be called ...       0    1     Bot\n",
       "243864  Woman and two young girls die after fire break...       0    1     Bot\n",
       "243865  Reasons not to always expect there's a budget ...       0    1     Bot\n",
       "243866  The girls defying all the odds to become scien...       0    1     Bot\n",
       "243867  Chancellor set to hit Britain’s strivers with ...       0    1     Bot\n",
       "243868  Emma Watson has addressed her criticism of Bey...       0    1     Bot\n",
       "243869  This year's Google Doodle celebrates 13 women ...       0    1     Bot\n",
       "243870  International Men’s Day was inaugurated in 199...       0    1     Bot\n",
       "243871  Germany dash England's hopes of lifting the Sh...       0    1     Bot\n",
       "243872  Booze Budget BOOM: Chancellor to SAVE our pubs...       0    1     Bot\n",
       "243873  Our view on the Lords and #Brexit: this is not...       0    1     Bot\n",
       "243874  TfL scraps standing only escalators - despite ...       0    1     Bot\n",
       "243875  Lovestruck Prince Harry 'mocked by pals after ...       0    1     Bot\n",
       "243876  WATCH: Moment Alexis Sanchez appears to LAUGH ...       0    1     Bot\n",
       "243877  Why is communication important? You asked Goog...       0    1     Bot\n",
       "243878  BREAKING: Killer avalanche leaves one dead and...       0    1     Bot\n",
       "243879  It's official. Donald Trump's inauguration cro...       0    1     Bot\n",
       "243880  #Emmerdale fans share extreme X-rated stories ...       0    1     Bot\n",
       "243881  Tomb of woman who helped discover DNA given li...       0    1     Bot\n",
       "243882  What do you know about UK housing? Take our qu...       0    1     Bot\n",
       "243883  Global investors brave China’s bad debt market...       0    1     Bot\n",
       "243884  Oh @piersmorgan not again! We feel for you @su...       0    1     Bot\n",
       "243885  Former couple @MrAshleyCain and @chloekhanxxx ...       0    1     Bot\n",
       "243886  BREAKING: Killer avalanche sweeps three skiers...       0    1     Bot\n",
       "243887  Why men should support International Women’s D...       0    1     Bot\n",
       "243888  How we can rebuild trust in a UK divided by in...       0    1     Bot\n",
       "243889  John Humphrys accused of patronising Angela Ra...       0    1     Bot\n",
       "243890  Fossilized poop found in 180-million-year-old ...       0    1     Bot\n",
       "\n",
       "[362458 rows x 4 columns]>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtd_tweettexts_df = pandas.DataFrame(rtd_tweettexts)\n",
    "ntd_tweettexts_df = pandas.DataFrame(ntd_tweettexts)\n",
    "\n",
    "rtd_tweettexts_df['Normal'] = 0\n",
    "rtd_tweettexts_df['Bot'] = 1\n",
    "rtd_tweettexts_df['Type'] = 'Bot'\n",
    "\n",
    "ntd_tweettexts_df['Normal'] = 1\n",
    "ntd_tweettexts_df['Bot'] = 0\n",
    "ntd_tweettexts_df['Type'] = 'Normal'\n",
    "\n",
    "ntd_tweettexts_df.columns = ['content', 'Normal','Bot','Type']\n",
    "print(ntd_tweettexts_df.columns)\n",
    "print(rtd_tweettexts_df.columns)\n",
    "\n",
    "all_tweettexts_df = ntd_tweettexts_df.append(rtd_tweettexts_df)\n",
    "all_tweettexts_df.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_tweettexts_df = pandas.read_csv('tweetData.csv')\n",
    "all_tweettexts_df = all_tweettexts_df.sample(frac=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create word Vector dataset\n",
    "\n",
    "def tweet2vec(tweettext):\n",
    "    tokens = preprocess(tweettext)\n",
    "    vec = tokenListVector(tokens)\n",
    "    return vec\n",
    "\n",
    "for x in range(0,30):\n",
    "    all_tweettexts_df['d'+str(x)] = 0\n",
    "\n",
    "vecMatrix = numpy.zeros((len(all_tweettexts_df),50+1))\n",
    "for i,row in all_tweettexts_df.iterrows():\n",
    "    if row['Type'] == \"Bot\":\n",
    "        vecMatrix[i][0] = int(1)\n",
    "    else:\n",
    "        vecMatrix[i][0] = int(0)\n",
    "    vec = tweet2vec(row['content'])\n",
    "    j = 1\n",
    "    for x in vec:\n",
    "        vecMatrix[i][j] = x\n",
    "        j+=1\n",
    "\n",
    "#all_tweettexts_df['vector'] = all_tweettexts_df['content'].apply(lambda x: tweet2vec(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Preprepare data 60:20:20\n",
    "test_size = .2\n",
    "sample_num = numpy.size(vecMatrix,0)\n",
    "numpy.random.shuffle(vecMatrix)\n",
    "\n",
    "train_x = vecMatrix[:int(test_size*sample_num),1:]\n",
    "train_y = vecMatrix[:int(test_size*sample_num),0]\n",
    "\n",
    "test_x = vecMatrix[int(test_size*sample_num)+1:,1:]\n",
    "test_y = vecMatrix[int(test_size*sample_num)+1:,0]\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "train_y = lab_enc.fit_transform(train_y)\n",
    "test_y = lab_enc.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.893349565121428\n"
     ]
    }
   ],
   "source": [
    "### SVM\n",
    "\n",
    "from sklearn import svm, metrics\n",
    "\n",
    "clf = svm.SVC(kernel='linear',gamma='auto')\n",
    "clf.fit(train_x, train_y) \n",
    "\n",
    "pred_y = clf.predict(test_x)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, pred_y))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "ym2O9vvY2MjH",
    "outputId": "ba1dab1a-e240-41ba-f812-d420fe53bd84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content', 'Normal', 'Bot', 'Type', 'vector'], dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CSE575_project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
