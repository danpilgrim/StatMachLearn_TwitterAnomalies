{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 362458 entries, 0 to 362457\n",
      "Data columns (total 3 columns):\n",
      "content    362458 non-null object\n",
      "Normal     362458 non-null int64\n",
      "Bot        362458 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 8.3+ MB\n",
      "None\n",
      "Index(['content', 'Normal', 'Bot'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 05:09:29,203 : INFO : collecting all words and their counts\n",
      "2018-11-20 05:09:29,210 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "2018-11-20 05:09:29,375 : INFO : PROGRESS: at sentence #10000, processed 144159 words, keeping 29469 word types\n",
      "2018-11-20 05:09:34,107 : INFO : PROGRESS: at sentence #20000, processed 280984 words, keeping 53366 word types\n",
      "2018-11-20 05:09:34,243 : INFO : PROGRESS: at sentence #30000, processed 418884 words, keeping 74614 word types\n",
      "2018-11-20 05:09:34,888 : INFO : PROGRESS: at sentence #40000, processed 559432 words, keeping 94459 word types\n",
      "2018-11-20 05:09:35,038 : INFO : PROGRESS: at sentence #50000, processed 705415 words, keeping 113280 word types\n",
      "2018-11-20 05:09:35,153 : INFO : PROGRESS: at sentence #60000, processed 855453 words, keeping 130722 word types\n",
      "2018-11-20 05:09:35,285 : INFO : PROGRESS: at sentence #70000, processed 1015560 words, keeping 146506 word types\n",
      "2018-11-20 05:09:35,422 : INFO : PROGRESS: at sentence #80000, processed 1176133 words, keeping 161819 word types\n",
      "2018-11-20 05:09:35,576 : INFO : PROGRESS: at sentence #90000, processed 1337547 words, keeping 176784 word types\n",
      "2018-11-20 05:09:35,766 : INFO : PROGRESS: at sentence #100000, processed 1499327 words, keeping 191499 word types\n",
      "2018-11-20 05:09:35,953 : INFO : PROGRESS: at sentence #110000, processed 1659526 words, keeping 205113 word types\n",
      "2018-11-20 05:09:36,091 : INFO : PROGRESS: at sentence #120000, processed 1802216 words, keeping 219912 word types\n",
      "2018-11-20 05:09:36,227 : INFO : PROGRESS: at sentence #130000, processed 1951870 words, keeping 235269 word types\n",
      "2018-11-20 05:09:36,360 : INFO : PROGRESS: at sentence #140000, processed 2113428 words, keeping 248892 word types\n",
      "2018-11-20 05:09:36,497 : INFO : PROGRESS: at sentence #150000, processed 2275647 words, keeping 262318 word types\n",
      "2018-11-20 05:09:36,628 : INFO : PROGRESS: at sentence #160000, processed 2438010 words, keeping 275710 word types\n",
      "2018-11-20 05:09:36,777 : INFO : PROGRESS: at sentence #170000, processed 2600898 words, keeping 288678 word types\n",
      "2018-11-20 05:09:36,941 : INFO : PROGRESS: at sentence #180000, processed 2738312 words, keeping 301013 word types\n",
      "2018-11-20 05:09:37,129 : INFO : PROGRESS: at sentence #190000, processed 2893508 words, keeping 316267 word types\n",
      "2018-11-20 05:09:37,294 : INFO : PROGRESS: at sentence #200000, processed 3086861 words, keeping 327202 word types\n",
      "2018-11-20 05:09:37,420 : INFO : PROGRESS: at sentence #210000, processed 3222430 words, keeping 336603 word types\n",
      "2018-11-20 05:09:37,598 : INFO : PROGRESS: at sentence #220000, processed 3383242 words, keeping 351944 word types\n",
      "2018-11-20 05:09:37,771 : INFO : PROGRESS: at sentence #230000, processed 3557199 words, keeping 365461 word types\n",
      "2018-11-20 05:09:37,913 : INFO : PROGRESS: at sentence #240000, processed 3697648 words, keeping 375622 word types\n",
      "2018-11-20 05:09:38,066 : INFO : PROGRESS: at sentence #250000, processed 3881241 words, keeping 389450 word types\n",
      "2018-11-20 05:09:38,230 : INFO : PROGRESS: at sentence #260000, processed 4059855 words, keeping 402264 word types\n",
      "2018-11-20 05:09:38,377 : INFO : PROGRESS: at sentence #270000, processed 4242100 words, keeping 415522 word types\n",
      "2018-11-20 05:09:38,487 : INFO : PROGRESS: at sentence #280000, processed 4413538 words, keeping 427734 word types\n",
      "2018-11-20 05:09:38,603 : INFO : PROGRESS: at sentence #290000, processed 4577065 words, keeping 440220 word types\n",
      "2018-11-20 05:09:38,715 : INFO : PROGRESS: at sentence #300000, processed 4679509 words, keeping 445783 word types\n",
      "2018-11-20 05:09:38,877 : INFO : PROGRESS: at sentence #310000, processed 4849261 words, keeping 458764 word types\n",
      "2018-11-20 05:09:39,038 : INFO : PROGRESS: at sentence #320000, processed 4989235 words, keeping 468570 word types\n",
      "2018-11-20 05:09:39,201 : INFO : PROGRESS: at sentence #330000, processed 5158355 words, keeping 481777 word types\n",
      "2018-11-20 05:09:39,332 : INFO : PROGRESS: at sentence #340000, processed 5306343 words, keeping 491803 word types\n",
      "2018-11-20 05:09:39,453 : INFO : PROGRESS: at sentence #350000, processed 5456103 words, keeping 500309 word types\n",
      "2018-11-20 05:09:39,582 : INFO : PROGRESS: at sentence #360000, processed 5620725 words, keeping 515760 word types\n",
      "2018-11-20 05:09:39,624 : INFO : collected 518836 word types from a corpus of 5660126 raw words and 362458 sentences\n",
      "2018-11-20 05:09:39,628 : INFO : Loading a fresh vocabulary\n",
      "2018-11-20 05:09:54,311 : INFO : effective_min_count=1 retains 518836 unique words (100% of original 518836, drops 0)\n",
      "2018-11-20 05:09:54,313 : INFO : effective_min_count=1 leaves 5660126 word corpus (100% of original 5660126, drops 0)\n",
      "2018-11-20 05:10:00,238 : INFO : deleting the raw counts dictionary of 518836 items\n",
      "2018-11-20 05:10:00,279 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2018-11-20 05:10:00,284 : INFO : downsampling leaves estimated 4571766 word corpus (80.8% of prior 5660126)\n",
      "2018-11-20 05:10:03,397 : INFO : estimated required memory for 518836 words and 100 dimensions: 674486800 bytes\n",
      "2018-11-20 05:10:03,400 : INFO : resetting layer weights\n",
      "2018-11-20 05:10:29,084 : INFO : training model with 10 workers on 518836 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-20 05:10:32,828 : INFO : EPOCH 1 - PROGRESS: at 0.17% examples, 5884 words/s, in_qsize 0, out_qsize 6\n",
      "2018-11-20 05:10:32,974 : INFO : EPOCH 1 - PROGRESS: at 0.35% examples, 4142 words/s, in_qsize 0, out_qsize 7\n",
      "2018-11-20 05:10:34,403 : INFO : EPOCH 1 - PROGRESS: at 3.29% examples, 26120 words/s, in_qsize 1, out_qsize 0\n",
      "2018-11-20 05:10:35,465 : INFO : EPOCH 1 - PROGRESS: at 5.90% examples, 38434 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:10:36,531 : INFO : EPOCH 1 - PROGRESS: at 13.31% examples, 74402 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:10:37,556 : INFO : EPOCH 1 - PROGRESS: at 19.03% examples, 95534 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:10:38,660 : INFO : EPOCH 1 - PROGRESS: at 22.29% examples, 100128 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:10:39,763 : INFO : EPOCH 1 - PROGRESS: at 27.58% examples, 112687 words/s, in_qsize 20, out_qsize 2\n",
      "2018-11-20 05:10:40,787 : INFO : EPOCH 1 - PROGRESS: at 33.42% examples, 124730 words/s, in_qsize 16, out_qsize 3\n",
      "2018-11-20 05:10:41,815 : INFO : EPOCH 1 - PROGRESS: at 40.76% examples, 140757 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:10:42,815 : INFO : EPOCH 1 - PROGRESS: at 47.45% examples, 152966 words/s, in_qsize 17, out_qsize 2\n",
      "2018-11-20 05:10:43,862 : INFO : EPOCH 1 - PROGRESS: at 53.95% examples, 163056 words/s, in_qsize 17, out_qsize 2\n",
      "2018-11-20 05:10:44,936 : INFO : EPOCH 1 - PROGRESS: at 60.60% examples, 171035 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:10:45,958 : INFO : EPOCH 1 - PROGRESS: at 66.64% examples, 177979 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:10:46,960 : INFO : EPOCH 1 - PROGRESS: at 71.37% examples, 182491 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:10:47,999 : INFO : EPOCH 1 - PROGRESS: at 77.77% examples, 190526 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:10:49,032 : INFO : EPOCH 1 - PROGRESS: at 84.87% examples, 195426 words/s, in_qsize 19, out_qsize 4\n",
      "2018-11-20 05:10:50,090 : INFO : EPOCH 1 - PROGRESS: at 92.20% examples, 202043 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:10:51,127 : INFO : EPOCH 1 - PROGRESS: at 98.39% examples, 205367 words/s, in_qsize 10, out_qsize 0\n",
      "2018-11-20 05:10:51,185 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 05:10:51,231 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 05:10:51,234 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 05:10:51,242 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 05:10:51,250 : INFO : worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 05:10:51,256 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 05:10:51,259 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 05:10:51,261 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 05:10:51,274 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 05:10:51,288 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 05:10:51,293 : INFO : EPOCH - 1 : training on 5660126 raw words (4573399 effective words) took 22.1s, 207067 effective words/s\n",
      "2018-11-20 05:10:52,361 : INFO : EPOCH 2 - PROGRESS: at 5.69% examples, 224857 words/s, in_qsize 18, out_qsize 0\n",
      "2018-11-20 05:10:53,369 : INFO : EPOCH 2 - PROGRESS: at 11.79% examples, 235567 words/s, in_qsize 20, out_qsize 1\n",
      "2018-11-20 05:10:54,419 : INFO : EPOCH 2 - PROGRESS: at 19.72% examples, 267949 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:10:55,480 : INFO : EPOCH 2 - PROGRESS: at 26.56% examples, 274934 words/s, in_qsize 19, out_qsize 1\n",
      "2018-11-20 05:10:56,481 : INFO : EPOCH 2 - PROGRESS: at 30.91% examples, 259711 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:10:57,500 : INFO : EPOCH 2 - PROGRESS: at 36.50% examples, 255657 words/s, in_qsize 20, out_qsize 1\n",
      "2018-11-20 05:10:58,528 : INFO : EPOCH 2 - PROGRESS: at 42.44% examples, 257218 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:10:59,543 : INFO : EPOCH 2 - PROGRESS: at 48.36% examples, 258216 words/s, in_qsize 20, out_qsize 2\n",
      "2018-11-20 05:11:00,587 : INFO : EPOCH 2 - PROGRESS: at 54.01% examples, 258020 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:01,600 : INFO : EPOCH 2 - PROGRESS: at 60.60% examples, 261808 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:02,723 : INFO : EPOCH 2 - PROGRESS: at 67.54% examples, 265911 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:03,733 : INFO : EPOCH 2 - PROGRESS: at 73.62% examples, 271161 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:04,803 : INFO : EPOCH 2 - PROGRESS: at 81.01% examples, 275825 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:05,927 : INFO : EPOCH 2 - PROGRESS: at 89.60% examples, 280305 words/s, in_qsize 20, out_qsize 1\n",
      "2018-11-20 05:11:06,963 : INFO : EPOCH 2 - PROGRESS: at 96.58% examples, 282497 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:07,242 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 05:11:07,277 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 05:11:07,280 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 05:11:07,282 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 05:11:07,333 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 05:11:07,341 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 05:11:07,348 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 05:11:07,366 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 05:11:07,389 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 05:11:07,419 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 05:11:07,422 : INFO : EPOCH - 2 : training on 5660126 raw words (4571465 effective words) took 16.1s, 284051 effective words/s\n",
      "2018-11-20 05:11:08,481 : INFO : EPOCH 3 - PROGRESS: at 5.70% examples, 227406 words/s, in_qsize 18, out_qsize 2\n",
      "2018-11-20 05:11:09,482 : INFO : EPOCH 3 - PROGRESS: at 13.68% examples, 277292 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:11:10,489 : INFO : EPOCH 3 - PROGRESS: at 20.93% examples, 291633 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:11,661 : INFO : EPOCH 3 - PROGRESS: at 27.92% examples, 286727 words/s, in_qsize 16, out_qsize 4\n",
      "2018-11-20 05:11:12,672 : INFO : EPOCH 3 - PROGRESS: at 35.63% examples, 295257 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:13,679 : INFO : EPOCH 3 - PROGRESS: at 42.95% examples, 301583 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:11:14,684 : INFO : EPOCH 3 - PROGRESS: at 51.11% examples, 309640 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:11:15,687 : INFO : EPOCH 3 - PROGRESS: at 57.07% examples, 307220 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:16,700 : INFO : EPOCH 3 - PROGRESS: at 63.31% examples, 306300 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:11:17,736 : INFO : EPOCH 3 - PROGRESS: at 69.67% examples, 306202 words/s, in_qsize 20, out_qsize 1\n",
      "2018-11-20 05:11:18,807 : INFO : EPOCH 3 - PROGRESS: at 76.00% examples, 307440 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:11:19,844 : INFO : EPOCH 3 - PROGRESS: at 83.34% examples, 306804 words/s, in_qsize 16, out_qsize 3\n",
      "2018-11-20 05:11:20,861 : INFO : EPOCH 3 - PROGRESS: at 90.99% examples, 310404 words/s, in_qsize 18, out_qsize 5\n",
      "2018-11-20 05:11:21,669 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 05:11:21,683 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 05:11:21,707 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 05:11:21,717 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 05:11:21,734 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 05:11:21,755 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 05:11:21,758 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 05:11:21,782 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 05:11:21,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 05:11:21,791 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 05:11:21,794 : INFO : EPOCH - 3 : training on 5660126 raw words (4571277 effective words) took 14.3s, 318925 effective words/s\n",
      "2018-11-20 05:11:22,832 : INFO : EPOCH 4 - PROGRESS: at 8.09% examples, 326192 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:11:23,866 : INFO : EPOCH 4 - PROGRESS: at 16.96% examples, 344016 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:24,869 : INFO : EPOCH 4 - PROGRESS: at 24.34% examples, 341402 words/s, in_qsize 19, out_qsize 1\n",
      "2018-11-20 05:11:25,922 : INFO : EPOCH 4 - PROGRESS: at 31.48% examples, 332374 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:26,985 : INFO : EPOCH 4 - PROGRESS: at 38.37% examples, 322342 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:11:27,988 : INFO : EPOCH 4 - PROGRESS: at 46.16% examples, 328330 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:29,013 : INFO : EPOCH 4 - PROGRESS: at 53.34% examples, 326827 words/s, in_qsize 18, out_qsize 3\n",
      "2018-11-20 05:11:30,044 : INFO : EPOCH 4 - PROGRESS: at 60.60% examples, 327178 words/s, in_qsize 20, out_qsize 2\n",
      "2018-11-20 05:11:31,131 : INFO : EPOCH 4 - PROGRESS: at 66.50% examples, 319357 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:32,133 : INFO : EPOCH 4 - PROGRESS: at 70.91% examples, 311732 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:33,170 : INFO : EPOCH 4 - PROGRESS: at 76.32% examples, 308957 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:34,193 : INFO : EPOCH 4 - PROGRESS: at 84.87% examples, 313105 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:11:35,198 : INFO : EPOCH 4 - PROGRESS: at 91.68% examples, 313513 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:11:36,057 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 05:11:36,134 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 05:11:36,141 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 05:11:36,151 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 05:11:36,165 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 05:11:36,195 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 05:11:36,202 : INFO : EPOCH 4 - PROGRESS: at 99.50% examples, 316353 words/s, in_qsize 3, out_qsize 1\n",
      "2018-11-20 05:11:36,205 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 05:11:36,212 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 05:11:36,247 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 05:11:36,284 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 05:11:36,290 : INFO : EPOCH - 4 : training on 5660126 raw words (4572075 effective words) took 14.5s, 316006 effective words/s\n",
      "2018-11-20 05:11:37,461 : INFO : EPOCH 5 - PROGRESS: at 6.89% examples, 249041 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:38,506 : INFO : EPOCH 5 - PROGRESS: at 13.50% examples, 254466 words/s, in_qsize 20, out_qsize 1\n",
      "2018-11-20 05:11:39,538 : INFO : EPOCH 5 - PROGRESS: at 20.75% examples, 273236 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:40,629 : INFO : EPOCH 5 - PROGRESS: at 28.25% examples, 284119 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:41,629 : INFO : EPOCH 5 - PROGRESS: at 35.79% examples, 292132 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:42,659 : INFO : EPOCH 5 - PROGRESS: at 43.63% examples, 301506 words/s, in_qsize 19, out_qsize 3\n",
      "2018-11-20 05:11:43,675 : INFO : EPOCH 5 - PROGRESS: at 52.01% examples, 310377 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:44,699 : INFO : EPOCH 5 - PROGRESS: at 59.44% examples, 314717 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:45,720 : INFO : EPOCH 5 - PROGRESS: at 67.40% examples, 321987 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:46,727 : INFO : EPOCH 5 - PROGRESS: at 74.73% examples, 329325 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:11:47,745 : INFO : EPOCH 5 - PROGRESS: at 83.70% examples, 334415 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:11:48,778 : INFO : EPOCH 5 - PROGRESS: at 91.50% examples, 336299 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:11:49,657 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 05:11:49,696 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 05:11:49,700 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 05:11:49,702 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 05:11:49,703 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 05:11:49,705 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 05:11:49,737 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 05:11:49,744 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 05:11:49,774 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 05:11:49,791 : INFO : EPOCH 5 - PROGRESS: at 100.00% examples, 339727 words/s, in_qsize 0, out_qsize 1\n",
      "2018-11-20 05:11:49,793 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 05:11:49,794 : INFO : EPOCH - 5 : training on 5660126 raw words (4571599 effective words) took 13.5s, 339641 effective words/s\n",
      "2018-11-20 05:11:50,905 : INFO : EPOCH 6 - PROGRESS: at 8.68% examples, 328863 words/s, in_qsize 20, out_qsize 1\n",
      "2018-11-20 05:11:51,983 : INFO : EPOCH 6 - PROGRESS: at 17.83% examples, 345043 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:11:52,985 : INFO : EPOCH 6 - PROGRESS: at 25.37% examples, 344795 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:11:54,003 : INFO : EPOCH 6 - PROGRESS: at 33.03% examples, 342232 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:11:55,076 : INFO : EPOCH 6 - PROGRESS: at 40.59% examples, 336833 words/s, in_qsize 18, out_qsize 2\n",
      "2018-11-20 05:11:56,094 : INFO : EPOCH 6 - PROGRESS: at 49.62% examples, 346934 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:11:57,131 : INFO : EPOCH 6 - PROGRESS: at 57.40% examples, 347579 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:58,183 : INFO : EPOCH 6 - PROGRESS: at 63.92% examples, 342827 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:11:59,187 : INFO : EPOCH 6 - PROGRESS: at 70.30% examples, 339930 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:00,211 : INFO : EPOCH 6 - PROGRESS: at 76.32% examples, 337743 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:01,214 : INFO : EPOCH 6 - PROGRESS: at 84.48% examples, 338814 words/s, in_qsize 20, out_qsize 3\n",
      "2018-11-20 05:12:02,229 : INFO : EPOCH 6 - PROGRESS: at 92.20% examples, 340193 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:12:03,016 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 05:12:03,046 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 05:12:03,047 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 05:12:03,055 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 05:12:03,123 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 05:12:03,154 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 05:12:03,157 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 05:12:03,160 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 05:12:03,177 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 05:12:03,183 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 05:12:03,186 : INFO : EPOCH - 6 : training on 5660126 raw words (4571606 effective words) took 13.4s, 342317 effective words/s\n",
      "2018-11-20 05:12:04,277 : INFO : EPOCH 7 - PROGRESS: at 7.29% examples, 279895 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:05,315 : INFO : EPOCH 7 - PROGRESS: at 16.96% examples, 335141 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:12:06,319 : INFO : EPOCH 7 - PROGRESS: at 25.20% examples, 347983 words/s, in_qsize 20, out_qsize 1\n",
      "2018-11-20 05:12:07,363 : INFO : EPOCH 7 - PROGRESS: at 32.26% examples, 336536 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:08,366 : INFO : EPOCH 7 - PROGRESS: at 41.26% examples, 349068 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:09,421 : INFO : EPOCH 7 - PROGRESS: at 48.19% examples, 340466 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:12:10,474 : INFO : EPOCH 7 - PROGRESS: at 54.81% examples, 335625 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:11,494 : INFO : EPOCH 7 - PROGRESS: at 61.09% examples, 327911 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:12:12,497 : INFO : EPOCH 7 - PROGRESS: at 67.39% examples, 325565 words/s, in_qsize 19, out_qsize 1\n",
      "2018-11-20 05:12:13,518 : INFO : EPOCH 7 - PROGRESS: at 73.35% examples, 324906 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:12:14,630 : INFO : EPOCH 7 - PROGRESS: at 80.02% examples, 322661 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:12:15,654 : INFO : EPOCH 7 - PROGRESS: at 88.75% examples, 325247 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:16,661 : INFO : EPOCH 7 - PROGRESS: at 94.94% examples, 322796 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:17,208 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 05:12:17,213 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 05:12:17,214 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 05:12:17,218 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 05:12:17,238 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 05:12:17,246 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 05:12:17,248 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 05:12:17,268 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 05:12:17,298 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 05:12:17,305 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 05:12:17,308 : INFO : EPOCH - 7 : training on 5660126 raw words (4570834 effective words) took 14.1s, 324380 effective words/s\n",
      "2018-11-20 05:12:18,340 : INFO : EPOCH 8 - PROGRESS: at 6.29% examples, 254060 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:12:19,348 : INFO : EPOCH 8 - PROGRESS: at 14.24% examples, 289738 words/s, in_qsize 19, out_qsize 1\n",
      "2018-11-20 05:12:20,385 : INFO : EPOCH 8 - PROGRESS: at 22.47% examples, 312307 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:21,392 : INFO : EPOCH 8 - PROGRESS: at 30.91% examples, 329555 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 05:12:22,404 : INFO : EPOCH 8 - PROGRESS: at 39.05% examples, 334232 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:23,444 : INFO : EPOCH 8 - PROGRESS: at 47.77% examples, 342852 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:24,485 : INFO : EPOCH 8 - PROGRESS: at 54.91% examples, 340544 words/s, in_qsize 20, out_qsize 4\n",
      "2018-11-20 05:12:25,544 : INFO : EPOCH 8 - PROGRESS: at 63.31% examples, 344610 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:26,594 : INFO : EPOCH 8 - PROGRESS: at 71.99% examples, 353204 words/s, in_qsize 17, out_qsize 2\n",
      "2018-11-20 05:12:27,597 : INFO : EPOCH 8 - PROGRESS: at 78.90% examples, 354200 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:12:28,633 : INFO : EPOCH 8 - PROGRESS: at 88.06% examples, 355089 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:12:29,654 : INFO : EPOCH 8 - PROGRESS: at 95.68% examples, 355015 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:12:29,930 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 05:12:29,965 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 05:12:30,018 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 05:12:30,056 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 05:12:30,078 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 05:12:30,081 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 05:12:30,097 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 05:12:30,101 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 05:12:30,116 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 05:12:30,123 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 05:12:30,126 : INFO : EPOCH - 8 : training on 5660126 raw words (4572041 effective words) took 12.8s, 357319 effective words/s\n",
      "2018-11-20 05:12:31,199 : INFO : EPOCH 9 - PROGRESS: at 7.89% examples, 313616 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:12:32,224 : INFO : EPOCH 9 - PROGRESS: at 16.44% examples, 331879 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:33,325 : INFO : EPOCH 9 - PROGRESS: at 25.54% examples, 347796 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:34,346 : INFO : EPOCH 9 - PROGRESS: at 35.09% examples, 363333 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:35,354 : INFO : EPOCH 9 - PROGRESS: at 42.62% examples, 359218 words/s, in_qsize 17, out_qsize 2\n",
      "2018-11-20 05:12:36,361 : INFO : EPOCH 9 - PROGRESS: at 51.63% examples, 365741 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:12:37,392 : INFO : EPOCH 9 - PROGRESS: at 58.59% examples, 359233 words/s, in_qsize 17, out_qsize 2\n",
      "2018-11-20 05:12:38,471 : INFO : EPOCH 9 - PROGRESS: at 66.50% examples, 358329 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:12:39,473 : INFO : EPOCH 9 - PROGRESS: at 73.04% examples, 358246 words/s, in_qsize 16, out_qsize 3\n",
      "2018-11-20 05:12:40,477 : INFO : EPOCH 9 - PROGRESS: at 80.01% examples, 357555 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 05:12:41,544 : INFO : EPOCH 9 - PROGRESS: at 88.75% examples, 355888 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:12:42,568 : INFO : EPOCH 9 - PROGRESS: at 97.23% examples, 358826 words/s, in_qsize 17, out_qsize 0\n",
      "2018-11-20 05:12:42,714 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 05:12:42,759 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 05:12:42,762 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 05:12:42,765 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 05:12:42,768 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 05:12:42,771 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 05:12:42,786 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 05:12:42,808 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 05:12:42,812 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 05:12:42,855 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 05:12:42,859 : INFO : EPOCH - 9 : training on 5660126 raw words (4571774 effective words) took 12.7s, 360440 effective words/s\n",
      "2018-11-20 05:12:43,939 : INFO : EPOCH 10 - PROGRESS: at 6.90% examples, 269728 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:12:44,958 : INFO : EPOCH 10 - PROGRESS: at 15.52% examples, 310759 words/s, in_qsize 13, out_qsize 6\n",
      "2018-11-20 05:12:45,975 : INFO : EPOCH 10 - PROGRESS: at 24.68% examples, 343210 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:46,983 : INFO : EPOCH 10 - PROGRESS: at 32.25% examples, 341685 words/s, in_qsize 20, out_qsize 2\n",
      "2018-11-20 05:12:47,987 : INFO : EPOCH 10 - PROGRESS: at 39.92% examples, 341037 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:48,989 : INFO : EPOCH 10 - PROGRESS: at 47.59% examples, 342981 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:12:50,037 : INFO : EPOCH 10 - PROGRESS: at 54.02% examples, 334794 words/s, in_qsize 14, out_qsize 5\n",
      "2018-11-20 05:12:51,056 : INFO : EPOCH 10 - PROGRESS: at 61.74% examples, 336940 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:52,083 : INFO : EPOCH 10 - PROGRESS: at 68.75% examples, 337266 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:12:53,095 : INFO : EPOCH 10 - PROGRESS: at 74.72% examples, 335720 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 05:12:54,098 : INFO : EPOCH 10 - PROGRESS: at 82.16% examples, 334971 words/s, in_qsize 18, out_qsize 0\n",
      "2018-11-20 05:12:55,101 : INFO : EPOCH 10 - PROGRESS: at 90.24% examples, 337672 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 05:12:56,125 : INFO : EPOCH 10 - PROGRESS: at 97.37% examples, 336908 words/s, in_qsize 11, out_qsize 5\n",
      "2018-11-20 05:12:56,222 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 05:12:56,239 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 05:12:56,243 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 05:12:56,263 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 05:12:56,273 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 05:12:56,275 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 05:12:56,296 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 05:12:56,320 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 05:12:56,326 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 05:12:56,351 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 05:12:56,354 : INFO : EPOCH - 10 : training on 5660126 raw words (4572459 effective words) took 13.5s, 339850 effective words/s\n",
      "2018-11-20 05:12:56,357 : INFO : training on a 56601260 raw words (45718529 effective words) took 147.3s, 310450 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45718529, 56601260)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import gensim \n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# load the data\n",
    "data = pd.read_csv('../data/classified_tweets/tweetData.csv', index_col=None, encoding='ISO-8859-1')\n",
    "print(data.info())\n",
    "\n",
    "# clean up columns\n",
    "data.columns = data.columns.str.strip() \n",
    "print(data.columns)\n",
    "\n",
    "# preprocess things\n",
    "document = []\n",
    "for line in data.content:\n",
    "    tokenized = gensim.utils.simple_preprocess(line)\n",
    "    document.append(tokenized)\n",
    "\n",
    "print(document)\n",
    "\n",
    "# if model doesn't exist, save it. Otherwise, train a new one\n",
    "model_name = \"word2vec.model\"\n",
    "wv_name = \"model.wv\"\n",
    "pModel = os.path.join(os.getcwd(), model_name)\n",
    "pWV = os.path.join(os.getcwd(), wv_name)\n",
    "\n",
    "#print(pModel)\n",
    "#print(pWV)\n",
    "\n",
    "#model = gensim.models.Word2Vec(document, size=150, window=10, min_count=2, workers=10)\n",
    "model = gensim.models.Word2Vec(min_count=1, workers=10)\n",
    "#if os.path.isfile(pModel):\n",
    "#    print(\"Loading model from file...\")\n",
    "#    model = gensim.models.Word2Vec.load(pModel)\n",
    "#    model.wv = gensim.models.KeyedVectors.load(pWV, mmap='r')\n",
    "#else:\n",
    "    # build vocabulary and train model\n",
    "#    print(\"Training new model...\")\n",
    "model.build_vocab(document)\n",
    "model.train(document, total_examples=len(document), epochs=10)\n",
    "#    model.save(pModel)\n",
    "#    model.wv.save(pWV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=518836, size=100, alpha=0.025)\n",
      "518836\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "vocab_size = len(model.wv.vocab)\n",
    "vocab_dim = 100\n",
    "print(vocab_size)\n",
    "print(vocab_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(518836, 100)\n"
     ]
    }
   ],
   "source": [
    "# convert the wv word vectors into a numpy matrix\n",
    "embedding_matrix = np.zeros((len(model.wv.vocab), vocab_dim))\n",
    "for i in range(len(model.wv.vocab)):\n",
    "    embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "c:\\python37\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89484972 0.85570252 0.88904023 ... 0.         0.         0.        ]\n",
      " [0.42072365 0.99999994 0.62941629 ... 0.         0.         0.        ]\n",
      " [0.79802865 0.99999994 0.5514912  ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.29221413 0.2368103  0.11814529 ... 0.         0.         0.        ]\n",
      " [0.47067466 0.30196905 0.29082671 ... 0.         0.         0.        ]\n",
      " [0.64416546 0.33563864 0.43536514 ... 0.         0.         0.        ]]\n",
      "(362458, 100)\n"
     ]
    }
   ],
   "source": [
    "# create matrix of similarities\n",
    "length = len(document)\n",
    "vec = np.zeros((length, vocab_dim))\n",
    "print(document)\n",
    "for i in range(length):\n",
    "    for j in range(len(document[i])-1):\n",
    "        s = model.wv.similarity(document[i][j], document[i][j+1])\n",
    "        # ignore negative cosine similarities for simplicity\n",
    "        if s > 0:\n",
    "            vec[i,j] = s\n",
    "print(vec)\n",
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (217474, 100)\n",
      "Y_train: (217474,)\n",
      "X_test: (144984, 100)\n",
      "X_test: (144984,)\n"
     ]
    }
   ],
   "source": [
    "# split the data into 60% train and 40% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(vec, data.Normal, test_size=0.4)\n",
    "print(\"X_train: \" + str(X_train.shape))\n",
    "print(\"Y_train: \" + str(Y_train.shape))\n",
    "print(\"X_test: \" + str(X_test.shape))\n",
    "print(\"X_test: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217474, 100)\n"
     ]
    }
   ],
   "source": [
    "# not sure if this is needed?\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train_tf = tf_transformer.transform(X_train)\n",
    "print(X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5748772278320367\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#X = StandardScaler().fit_transform(embedding_matrix)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tf, Y_train)\n",
    "print(clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37430 38626]\n",
      " [23010 45918]]\n",
      "Precision % = 54.31254731264194\n",
      "Recall % = 66.61733983286908\n",
      "False Positive % = 26.64156044804944\n",
      "False Negative % = 15.870716768746897\n"
     ]
    }
   ],
   "source": [
    "# borrowed this technique from Daniel\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(Y_test, Y_hat)\n",
    "print(confusion)\n",
    "true_neg = float(confusion[0][0])\n",
    "true_pos = float(confusion[1][1])\n",
    "false_neg = float(confusion[1][0])\n",
    "false_pos = float(confusion[0][1])\n",
    "total = true_neg+true_pos+false_neg+false_pos\n",
    "\n",
    "print(\"Precision % = \" + str(true_pos*100/(true_pos+false_pos)))\n",
    "print(\"Recall % = \" + str(true_pos*100/(false_neg+true_pos)))\n",
    "print(\"False Positive % = \" + str(false_pos*100/total))\n",
    "print(\"False Negative % = \" + str(false_neg*100/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5865681730397837\n",
      "[[46235 29821]\n",
      " [30120 38808]]\n",
      "Precision % = 56.54752364160923\n",
      "Recall % = 56.30222841225627\n",
      "False Positive % = 20.568476521547204\n",
      "False Negative % = 20.774706174474424\n"
     ]
    }
   ],
   "source": [
    "# now do the logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, Y_train)\n",
    "print(lr.score(X_test, Y_test))\n",
    "\n",
    "Y_hat = lr.predict(X_test)\n",
    "confusion = confusion_matrix(Y_test, Y_hat)\n",
    "print(confusion)\n",
    "\n",
    "true_neg = float(confusion[0][0])\n",
    "true_pos = float(confusion[1][1])\n",
    "false_neg = float(confusion[1][0])\n",
    "false_pos = float(confusion[0][1])\n",
    "total = true_neg+true_pos+false_neg+false_pos\n",
    "\n",
    "print(\"Precision % = \" + str(true_pos*100/(true_pos+false_pos)))\n",
    "print(\"Recall % = \" + str(true_pos*100/(false_neg+true_pos)))\n",
    "print(\"False Positive % = \" + str(false_pos*100/total))\n",
    "print(\"False Negative % = \" + str(false_neg*100/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the svm\n",
    "from sklearn import svm\n",
    "\n",
    "s = svm.SVC(kernel='linear', gamma='auto').fit(X_train, Y_train)\n",
    "print(s.score(X_test, Y_test))\n",
    "\n",
    "Y_hat = s.predict(X_test)\n",
    "confusion = confusion_matrix(Y_test, Y_hat)\n",
    "print(confusion)\n",
    "\n",
    "true_neg = float(confusion[0][0])\n",
    "true_pos = float(confusion[1][1])\n",
    "false_neg = float(confusion[1][0])\n",
    "false_pos = float(confusion[0][1])\n",
    "total = true_neg+true_pos+false_neg+false_pos\n",
    "\n",
    "print(\"Precision % = \" + str(true_pos*100/(true_pos+false_pos)))\n",
    "print(\"Recall % = \" + str(true_pos*100/(false_neg+true_pos)))\n",
    "print(\"False Positive % = \" + str(false_pos*100/total))\n",
    "print(\"False Negative % = \" + str(false_neg*100/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3).fit(X_train, Y_train)\n",
    "print(knn.score(X_test, Y_test))\n",
    "\n",
    "Y_hat = knn.predict(X_test)\n",
    "confusion = confusion_matrix(Y_test, Y_hat)\n",
    "print(confusion)\n",
    "\n",
    "true_neg = float(confusion[0][0])\n",
    "true_pos = float(confusion[1][1])\n",
    "false_neg = float(confusion[1][0])\n",
    "false_pos = float(confusion[0][1])\n",
    "total = true_neg+true_pos+false_neg+false_pos\n",
    "\n",
    "print(\"Precision % = \" + str(true_pos*100/(true_pos+false_pos)))\n",
    "print(\"Recall % = \" + str(true_pos*100/(false_neg+true_pos)))\n",
    "print(\"False Positive % = \" + str(false_pos*100/total))\n",
    "print(\"False Negative % = \" + str(false_neg*100/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
