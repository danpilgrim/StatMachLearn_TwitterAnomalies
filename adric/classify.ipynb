{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 362458 entries, 0 to 362457\n",
      "Data columns (total 3 columns):\n",
      "content    362458 non-null object\n",
      "Normal     362458 non-null int64\n",
      "Bot        362458 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 8.3+ MB\n",
      "None\n",
      "Index(['content', 'Normal', 'Bot'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import gensim \n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# load the data\n",
    "data = pd.read_csv('../data/classified_tweets/tweetData.csv', index_col=None, encoding='ISO-8859-1')\n",
    "print(data.info())\n",
    "\n",
    "# clean up columns\n",
    "data.columns = data.columns.str.strip() \n",
    "print(data.columns)\n",
    "\n",
    "# preprocess things\n",
    "other_document = []\n",
    "troll_document = []\n",
    "for i in range(len(data.content)):\n",
    "    tokenized = gensim.utils.simple_preprocess(data.content[i])\n",
    "    if i < 172207:\n",
    "        other_document.append(tokenized)\n",
    "    else:\n",
    "        troll_document.append(tokenized)\n",
    "print(other_document)\n",
    "print(troll_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 07:40:10,591 : INFO : collecting all words and their counts\n",
      "2018-11-20 07:40:10,591 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-20 07:40:10,747 : INFO : PROGRESS: at sentence #10000, processed 144159 words, keeping 29469 word types\n",
      "2018-11-20 07:40:10,935 : INFO : PROGRESS: at sentence #20000, processed 280984 words, keeping 53366 word types\n",
      "2018-11-20 07:40:11,091 : INFO : PROGRESS: at sentence #30000, processed 418884 words, keeping 74614 word types\n",
      "2018-11-20 07:40:11,247 : INFO : PROGRESS: at sentence #40000, processed 559432 words, keeping 94459 word types\n",
      "2018-11-20 07:40:11,434 : INFO : PROGRESS: at sentence #50000, processed 705415 words, keeping 113280 word types\n",
      "2018-11-20 07:40:11,605 : INFO : PROGRESS: at sentence #60000, processed 855453 words, keeping 130722 word types\n",
      "2018-11-20 07:40:11,746 : INFO : PROGRESS: at sentence #70000, processed 1015560 words, keeping 146506 word types\n",
      "2018-11-20 07:40:11,902 : INFO : PROGRESS: at sentence #80000, processed 1176133 words, keeping 161819 word types\n",
      "2018-11-20 07:40:12,105 : INFO : PROGRESS: at sentence #90000, processed 1337547 words, keeping 176784 word types\n",
      "2018-11-20 07:40:12,276 : INFO : PROGRESS: at sentence #100000, processed 1499327 words, keeping 191499 word types\n",
      "2018-11-20 07:40:12,417 : INFO : PROGRESS: at sentence #110000, processed 1659526 words, keeping 205113 word types\n",
      "2018-11-20 07:40:12,588 : INFO : PROGRESS: at sentence #120000, processed 1802216 words, keeping 219912 word types\n",
      "2018-11-20 07:40:12,744 : INFO : PROGRESS: at sentence #130000, processed 1951870 words, keeping 235269 word types\n",
      "2018-11-20 07:40:12,869 : INFO : PROGRESS: at sentence #140000, processed 2113428 words, keeping 248892 word types\n",
      "2018-11-20 07:40:12,994 : INFO : PROGRESS: at sentence #150000, processed 2275647 words, keeping 262318 word types\n",
      "2018-11-20 07:40:13,119 : INFO : PROGRESS: at sentence #160000, processed 2438010 words, keeping 275710 word types\n",
      "2018-11-20 07:40:13,243 : INFO : PROGRESS: at sentence #170000, processed 2600898 words, keeping 288678 word types\n",
      "2018-11-20 07:40:13,275 : INFO : collected 291325 word types from a corpus of 2633586 raw words and 172207 sentences\n",
      "2018-11-20 07:40:13,275 : INFO : Loading a fresh vocabulary\n",
      "2018-11-20 07:40:15,272 : INFO : effective_min_count=1 retains 291325 unique words (100% of original 291325, drops 0)\n",
      "2018-11-20 07:40:15,272 : INFO : effective_min_count=1 leaves 2633586 word corpus (100% of original 2633586, drops 0)\n",
      "2018-11-20 07:40:18,517 : INFO : deleting the raw counts dictionary of 291325 items\n",
      "2018-11-20 07:40:18,533 : INFO : sample=0.001 downsamples 41 most-common words\n",
      "2018-11-20 07:40:18,548 : INFO : downsampling leaves estimated 2019975 word corpus (76.7% of prior 2633586)\n",
      "2018-11-20 07:40:19,063 : INFO : constructing a huffman tree from 291325 words\n",
      "2018-11-20 07:40:42,880 : INFO : built huffman tree with maximum node depth 22\n",
      "2018-11-20 07:40:43,114 : INFO : estimated required memory for 291325 words and 100 dimensions: 436987500 bytes\n",
      "2018-11-20 07:40:43,114 : INFO : resetting layer weights\n",
      "2018-11-20 07:40:55,633 : INFO : collecting all words and their counts\n",
      "2018-11-20 07:40:55,649 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-20 07:40:55,742 : INFO : PROGRESS: at sentence #10000, processed 135072 words, keeping 25249 word types\n",
      "2018-11-20 07:40:55,867 : INFO : PROGRESS: at sentence #20000, processed 298763 words, keeping 46224 word types\n",
      "2018-11-20 07:40:55,976 : INFO : PROGRESS: at sentence #30000, processed 475827 words, keeping 58994 word types\n",
      "2018-11-20 07:40:56,070 : INFO : PROGRESS: at sentence #40000, processed 621398 words, keeping 74536 word types\n",
      "2018-11-20 07:40:56,195 : INFO : PROGRESS: at sentence #50000, processed 786592 words, keeping 92474 word types\n",
      "2018-11-20 07:40:56,304 : INFO : PROGRESS: at sentence #60000, processed 963243 words, keeping 109296 word types\n",
      "2018-11-20 07:40:56,413 : INFO : PROGRESS: at sentence #70000, processed 1106422 words, keeping 120697 word types\n",
      "2018-11-20 07:40:56,553 : INFO : PROGRESS: at sentence #80000, processed 1287684 words, keeping 136171 word types\n",
      "2018-11-20 07:40:56,678 : INFO : PROGRESS: at sentence #90000, processed 1466863 words, keeping 150801 word types\n",
      "2018-11-20 07:40:56,787 : INFO : PROGRESS: at sentence #100000, processed 1645932 words, keeping 165322 word types\n",
      "2018-11-20 07:40:56,943 : INFO : PROGRESS: at sentence #110000, processed 1818404 words, keeping 178854 word types\n",
      "2018-11-20 07:40:57,037 : INFO : PROGRESS: at sentence #120000, processed 1962432 words, keeping 190891 word types\n",
      "2018-11-20 07:40:57,146 : INFO : PROGRESS: at sentence #130000, processed 2084868 words, keeping 198761 word types\n",
      "2018-11-20 07:40:57,240 : INFO : PROGRESS: at sentence #140000, processed 2239001 words, keeping 210667 word types\n",
      "2018-11-20 07:40:57,349 : INFO : PROGRESS: at sentence #150000, processed 2390726 words, keeping 223615 word types\n",
      "2018-11-20 07:40:57,474 : INFO : PROGRESS: at sentence #160000, processed 2560595 words, keeping 237782 word types\n",
      "2018-11-20 07:40:57,583 : INFO : PROGRESS: at sentence #170000, processed 2702524 words, keeping 247636 word types\n",
      "2018-11-20 07:40:57,708 : INFO : PROGRESS: at sentence #180000, processed 2858775 words, keeping 257376 word types\n",
      "2018-11-20 07:40:57,848 : INFO : PROGRESS: at sentence #190000, processed 3023134 words, keeping 274518 word types\n",
      "2018-11-20 07:40:57,848 : INFO : collected 274634 word types from a corpus of 3026540 raw words and 190251 sentences\n",
      "2018-11-20 07:40:57,848 : INFO : Loading a fresh vocabulary\n",
      "2018-11-20 07:40:59,502 : INFO : effective_min_count=1 retains 274634 unique words (100% of original 274634, drops 0)\n",
      "2018-11-20 07:40:59,502 : INFO : effective_min_count=1 leaves 3026540 word corpus (100% of original 3026540, drops 0)\n",
      "2018-11-20 07:41:02,481 : INFO : deleting the raw counts dictionary of 274634 items\n",
      "2018-11-20 07:41:02,481 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2018-11-20 07:41:02,481 : INFO : downsampling leaves estimated 2470821 word corpus (81.6% of prior 3026540)\n",
      "2018-11-20 07:41:02,996 : INFO : constructing a huffman tree from 274634 words\n",
      "2018-11-20 07:41:25,710 : INFO : built huffman tree with maximum node depth 22\n",
      "2018-11-20 07:41:25,913 : INFO : estimated required memory for 274634 words and 100 dimensions: 411951000 bytes\n",
      "2018-11-20 07:41:25,913 : INFO : resetting layer weights\n",
      "2018-11-20 07:41:39,596 : INFO : training model with 10 workers on 291325 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=0 window=5\n",
      "2018-11-20 07:41:40,735 : INFO : EPOCH 1 - PROGRESS: at 12.00% examples, 205648 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:41:41,811 : INFO : EPOCH 1 - PROGRESS: at 25.22% examples, 220867 words/s, in_qsize 16, out_qsize 4\n",
      "2018-11-20 07:41:42,903 : INFO : EPOCH 1 - PROGRESS: at 40.78% examples, 242010 words/s, in_qsize 13, out_qsize 0\n",
      "2018-11-20 07:41:44,042 : INFO : EPOCH 1 - PROGRESS: at 55.54% examples, 249897 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 07:41:45,103 : INFO : EPOCH 1 - PROGRESS: at 71.52% examples, 259979 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:41:46,117 : INFO : EPOCH 1 - PROGRESS: at 85.78% examples, 264999 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:41:46,881 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 07:41:46,913 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 07:41:46,928 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 07:41:47,022 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 07:41:47,053 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 07:41:47,053 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 07:41:47,069 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 07:41:47,069 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 07:41:47,069 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 07:41:47,084 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 07:41:47,084 : INFO : EPOCH - 1 : training on 2633586 raw words (2020354 effective words) took 7.5s, 270441 effective words/s\n",
      "2018-11-20 07:41:48,129 : INFO : EPOCH 2 - PROGRESS: at 12.00% examples, 227539 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 07:41:49,175 : INFO : EPOCH 2 - PROGRESS: at 22.82% examples, 212988 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:41:50,235 : INFO : EPOCH 2 - PROGRESS: at 37.53% examples, 234873 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 07:41:51,281 : INFO : EPOCH 2 - PROGRESS: at 51.58% examples, 245502 words/s, in_qsize 17, out_qsize 3\n",
      "2018-11-20 07:41:52,295 : INFO : EPOCH 2 - PROGRESS: at 66.67% examples, 257791 words/s, in_qsize 19, out_qsize 1\n",
      "2018-11-20 07:41:53,309 : INFO : EPOCH 2 - PROGRESS: at 81.47% examples, 263865 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:41:54,293 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 07:41:54,340 : INFO : EPOCH 2 - PROGRESS: at 97.15% examples, 271740 words/s, in_qsize 8, out_qsize 1\n",
      "2018-11-20 07:41:54,340 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 07:41:54,387 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 07:41:54,449 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 07:41:54,449 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 07:41:54,465 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 07:41:54,465 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 07:41:54,465 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 07:41:54,481 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 07:41:54,496 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 07:41:54,496 : INFO : EPOCH - 2 : training on 2633586 raw words (2019719 effective words) took 7.4s, 274128 effective words/s\n",
      "2018-11-20 07:41:55,573 : INFO : EPOCH 3 - PROGRESS: at 11.99% examples, 219112 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 07:41:56,571 : INFO : EPOCH 3 - PROGRESS: at 26.41% examples, 246820 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:41:57,616 : INFO : EPOCH 3 - PROGRESS: at 39.70% examples, 250352 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:41:58,724 : INFO : EPOCH 3 - PROGRESS: at 54.10% examples, 255634 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:41:59,769 : INFO : EPOCH 3 - PROGRESS: at 67.08% examples, 255612 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:42:00,892 : INFO : EPOCH 3 - PROGRESS: at 82.92% examples, 260804 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 07:42:01,735 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 07:42:01,797 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 07:42:01,828 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 07:42:01,828 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 07:42:01,828 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 07:42:01,891 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 07:42:01,906 : INFO : EPOCH 3 - PROGRESS: at 99.06% examples, 270872 words/s, in_qsize 3, out_qsize 1\n",
      "2018-11-20 07:42:01,906 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 07:42:01,937 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 07:42:01,937 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 07:42:01,953 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 07:42:01,953 : INFO : EPOCH - 3 : training on 2633586 raw words (2019865 effective words) took 7.4s, 271657 effective words/s\n",
      "2018-11-20 07:42:01,953 : INFO : training on a 7900758 raw words (6059938 effective words) took 22.4s, 270922 effective words/s\n",
      "2018-11-20 07:42:01,969 : INFO : training model with 10 workers on 274634 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=0 window=5\n",
      "2018-11-20 07:42:03,029 : INFO : EPOCH 1 - PROGRESS: at 11.34% examples, 270681 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:42:04,059 : INFO : EPOCH 1 - PROGRESS: at 23.86% examples, 281506 words/s, in_qsize 18, out_qsize 2\n",
      "2018-11-20 07:42:05,135 : INFO : EPOCH 1 - PROGRESS: at 38.02% examples, 299885 words/s, in_qsize 20, out_qsize 1\n",
      "2018-11-20 07:42:06,150 : INFO : EPOCH 1 - PROGRESS: at 47.65% examples, 292605 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:42:07,195 : INFO : EPOCH 1 - PROGRESS: at 60.95% examples, 301810 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:42:08,194 : INFO : EPOCH 1 - PROGRESS: at 73.65% examples, 296304 words/s, in_qsize 17, out_qsize 2\n",
      "2018-11-20 07:42:09,223 : INFO : EPOCH 1 - PROGRESS: at 85.00% examples, 293742 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:42:10,081 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 07:42:10,128 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 07:42:10,191 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 07:42:10,191 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 07:42:10,206 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 07:42:10,222 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 07:42:10,253 : INFO : EPOCH 1 - PROGRESS: at 99.04% examples, 297238 words/s, in_qsize 3, out_qsize 1\n",
      "2018-11-20 07:42:10,253 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 07:42:10,284 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 07:42:10,284 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 07:42:10,315 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 07:42:10,315 : INFO : EPOCH - 1 : training on 3026540 raw words (2470594 effective words) took 8.3s, 297639 effective words/s\n",
      "2018-11-20 07:42:11,362 : INFO : EPOCH 2 - PROGRESS: at 11.69% examples, 279473 words/s, in_qsize 16, out_qsize 0\n",
      "2018-11-20 07:42:12,391 : INFO : EPOCH 2 - PROGRESS: at 23.21% examples, 272880 words/s, in_qsize 18, out_qsize 2\n",
      "2018-11-20 07:42:13,468 : INFO : EPOCH 2 - PROGRESS: at 36.32% examples, 284503 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 07:42:14,497 : INFO : EPOCH 2 - PROGRESS: at 47.36% examples, 290493 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:42:15,511 : INFO : EPOCH 2 - PROGRESS: at 57.72% examples, 288627 words/s, in_qsize 18, out_qsize 1\n",
      "2018-11-20 07:42:16,541 : INFO : EPOCH 2 - PROGRESS: at 72.46% examples, 291577 words/s, in_qsize 18, out_qsize 2\n",
      "2018-11-20 07:42:17,586 : INFO : EPOCH 2 - PROGRESS: at 84.41% examples, 290586 words/s, in_qsize 20, out_qsize 2\n",
      "2018-11-20 07:42:18,647 : INFO : EPOCH 2 - PROGRESS: at 96.73% examples, 288609 words/s, in_qsize 6, out_qsize 8\n",
      "2018-11-20 07:42:18,647 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 07:42:18,647 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 07:42:18,647 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 07:42:18,647 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 07:42:18,662 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 07:42:18,740 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 07:42:18,740 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 07:42:18,756 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 07:42:18,756 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 07:42:18,772 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 07:42:18,787 : INFO : EPOCH - 2 : training on 3026540 raw words (2470324 effective words) took 8.4s, 292984 effective words/s\n",
      "2018-11-20 07:42:19,832 : INFO : EPOCH 3 - PROGRESS: at 9.07% examples, 205243 words/s, in_qsize 17, out_qsize 0\n",
      "2018-11-20 07:42:20,862 : INFO : EPOCH 3 - PROGRESS: at 21.33% examples, 252769 words/s, in_qsize 20, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 07:42:21,856 : INFO : EPOCH 3 - PROGRESS: at 31.97% examples, 258105 words/s, in_qsize 20, out_qsize 1\n",
      "2018-11-20 07:42:22,870 : INFO : EPOCH 3 - PROGRESS: at 42.70% examples, 265733 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:42:23,868 : INFO : EPOCH 3 - PROGRESS: at 52.66% examples, 268435 words/s, in_qsize 20, out_qsize 0\n",
      "2018-11-20 07:42:24,929 : INFO : EPOCH 3 - PROGRESS: at 64.36% examples, 267599 words/s, in_qsize 19, out_qsize 0\n",
      "2018-11-20 07:42:25,990 : INFO : EPOCH 3 - PROGRESS: at 76.77% examples, 266336 words/s, in_qsize 20, out_qsize 1\n",
      "2018-11-20 07:42:27,066 : INFO : EPOCH 3 - PROGRESS: at 88.75% examples, 268004 words/s, in_qsize 19, out_qsize 1\n",
      "2018-11-20 07:42:27,690 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-20 07:42:27,706 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-20 07:42:27,706 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-20 07:42:27,768 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-20 07:42:27,784 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-20 07:42:27,831 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-20 07:42:27,846 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-20 07:42:27,862 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 07:42:27,877 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 07:42:27,877 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 07:42:27,877 : INFO : EPOCH - 3 : training on 3026540 raw words (2470230 effective words) took 9.0s, 273073 effective words/s\n",
      "2018-11-20 07:42:27,893 : INFO : training on a 9079620 raw words (7411148 effective words) took 25.9s, 285892 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7411148, 9079620)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a model for each tweet type\n",
    "from copy import deepcopy\n",
    "\n",
    "model0 = gensim.models.Word2Vec(min_count=1, workers=10, hs=1, negative=0)\n",
    "model0.build_vocab(other_document)\n",
    "\n",
    "model1 = gensim.models.Word2Vec(min_count=1, workers=10, hs=1, negative=0)\n",
    "model1.build_vocab(troll_document)\n",
    "\n",
    "# now train each model\n",
    "model0.train(other_document, total_examples=len(other_document), epochs=3)\n",
    "model1.train(troll_document, total_examples=len(troll_document), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=291325, size=100, alpha=0.025)\n",
      "Word2Vec(vocab=274634, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "models = [model0, model1]\n",
    "print(models[0]) # other\n",
    "print(models[1]) # troll\n",
    "vocab0_size = len(models[0].wv.vocab)\n",
    "vocab1_size = len(models[0].wv.vocab)\n",
    "vocab_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 07:48:03,248 : INFO : scoring sentences with 10 workers on 291325 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 and negative=0\n",
      "2018-11-20 07:48:04,246 : INFO : PROGRESS: at 2780000.00% sentences, 27781 sentences/s\n",
      "2018-11-20 07:48:05,245 : INFO : PROGRESS: at 5400000.00% sentences, 26921 sentences/s\n",
      "2018-11-20 07:48:06,251 : INFO : PROGRESS: at 8060000.00% sentences, 26769 sentences/s\n",
      "2018-11-20 07:48:07,251 : INFO : PROGRESS: at 10940000.00% sentences, 27272 sentences/s\n",
      "2018-11-20 07:48:08,265 : INFO : PROGRESS: at 13860000.00% sentences, 27638 sentences/s\n",
      "2018-11-20 07:48:09,263 : INFO : PROGRESS: at 16730000.00% sentences, 27805 sentences/s\n",
      "2018-11-20 07:48:10,261 : INFO : PROGRESS: at 19750000.00% sentences, 28136 sentences/s\n",
      "2018-11-20 07:48:11,275 : INFO : PROGRESS: at 22450000.00% sentences, 27948 sentences/s\n",
      "2018-11-20 07:48:12,289 : INFO : PROGRESS: at 25330000.00% sentences, 28030 sentences/s\n",
      "2018-11-20 07:48:13,288 : INFO : PROGRESS: at 28060000.00% sentences, 27957 sentences/s\n",
      "2018-11-20 07:48:14,287 : INFO : PROGRESS: at 31150000.00% sentences, 28223 sentences/s\n",
      "2018-11-20 07:48:15,286 : INFO : PROGRESS: at 34060000.00% sentences, 28293 sentences/s\n",
      "2018-11-20 07:48:15,941 : INFO : reached end of input; waiting to finish 30 outstanding jobs\n",
      "2018-11-20 07:48:16,050 : INFO : scoring 362458 sentences took 12.8s, 28323 sentences/s\n",
      "2018-11-20 07:48:16,050 : INFO : scoring sentences with 10 workers on 274634 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 and negative=0\n",
      "2018-11-20 07:48:17,048 : INFO : PROGRESS: at 3010000.00% sentences, 30014 sentences/s\n",
      "2018-11-20 07:48:18,062 : INFO : PROGRESS: at 5820000.00% sentences, 28936 sentences/s\n",
      "2018-11-20 07:48:19,061 : INFO : PROGRESS: at 8450000.00% sentences, 28033 sentences/s\n",
      "2018-11-20 07:48:20,059 : INFO : PROGRESS: at 11180000.00% sentences, 27850 sentences/s\n",
      "2018-11-20 07:48:21,063 : INFO : PROGRESS: at 14110000.00% sentences, 28128 sentences/s\n",
      "2018-11-20 07:48:22,063 : INFO : PROGRESS: at 16800000.00% sentences, 27917 sentences/s\n",
      "2018-11-20 07:48:23,077 : INFO : PROGRESS: at 19510000.00% sentences, 27768 sentences/s\n",
      "2018-11-20 07:48:24,077 : INFO : PROGRESS: at 22390000.00% sentences, 27870 sentences/s\n",
      "2018-11-20 07:48:25,091 : INFO : PROGRESS: at 25120000.00% sentences, 27795 sentences/s\n",
      "2018-11-20 07:48:26,090 : INFO : PROGRESS: at 27690000.00% sentences, 27583 sentences/s\n",
      "2018-11-20 07:48:27,088 : INFO : PROGRESS: at 30540000.00% sentences, 27664 sentences/s\n",
      "2018-11-20 07:48:28,086 : INFO : PROGRESS: at 33320000.00% sentences, 27658 sentences/s\n",
      "2018-11-20 07:48:29,069 : INFO : reached end of input; waiting to finish 29 outstanding jobs\n",
      "2018-11-20 07:48:29,116 : INFO : PROGRESS: at 35970000.00% sentences, 27521 sentences/s\n",
      "2018-11-20 07:48:29,178 : INFO : scoring 362458 sentences took 13.1s, 27596 sentences/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1\n",
      "doc                    \n",
      "0    0.467345  0.532655\n",
      "1    0.562522  0.437478\n"
     ]
    }
   ],
   "source": [
    "# based on sample code from gensim:\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/deepir.ipynb\n",
    "docs = [other_document, troll_document]\n",
    "listall = [s for d in docs for s in d]\n",
    "# the log likelihood of each sentence in this category under each w2v representation\n",
    "llhd = np.array([ m.score(listall, len(listall)) for m in models ])\n",
    "# now exponentiate to get likelihoods, \n",
    "lhd = np.exp(llhd - llhd.max(axis=0)) # subtract row max to avoid numeric overload\n",
    "# normalize across models to get sentence-category probabilities\n",
    "prob = pd.DataFrame((lhd/lhd.sum(axis=0)).transpose())\n",
    "# and finally average the sentence probabilities to get the category's probability\n",
    "prob[\"doc\"] = [i for i,d in enumerate(docs) for s in d]\n",
    "prob = prob.groupby(\"doc\").mean()\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 2.7998937e-03 4.6160036e-08 ... 1.6678620e-09\n",
      "  1.0000000e+00 1.9203770e-03]\n",
      " [1.1070258e-42 1.0000000e+00 1.0000000e+00 ... 1.0000000e+00\n",
      "  1.5398059e-14 1.0000000e+00]]\n",
      "(2, 362458)\n"
     ]
    }
   ],
   "source": [
    "vec = lhd.clip(min=0) # remove any negative values\n",
    "print(vec)\n",
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (217474, 2)\n",
      "Y_train: (217474,)\n",
      "X_test: (144984, 2)\n",
      "X_test: (144984,)\n"
     ]
    }
   ],
   "source": [
    "# split the data into 60% train and 40% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(vec.transpose(), data.Normal, test_size=0.4)\n",
    "print(\"X_train: \" + str(X_train.shape))\n",
    "print(\"Y_train: \" + str(Y_train.shape))\n",
    "print(\"X_test: \" + str(X_test.shape))\n",
    "print(\"X_test: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217474, 2)\n"
     ]
    }
   ],
   "source": [
    "# not sure if this is needed?\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train_tf = tf_transformer.transform(X_train)\n",
    "print(X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5512263422170722\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#X = StandardScaler().fit_transform(embedding_matrix)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43933 32267]\n",
      " [32798 35986]]\n",
      "Precision % = 52.724422369712684\n",
      "Recall % = 52.31739939520819\n",
      "False Positive % = 22.255559234122387\n",
      "False Negative % = 22.62180654417039\n"
     ]
    }
   ],
   "source": [
    "# borrowed this technique from Daniel\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_hat = clf.predict(X_test)\n",
    "confusion = confusion_matrix(Y_test, Y_hat)\n",
    "print(confusion)\n",
    "true_neg = float(confusion[0][0])\n",
    "true_pos = float(confusion[1][1])\n",
    "false_neg = float(confusion[1][0])\n",
    "false_pos = float(confusion[0][1])\n",
    "total = true_neg+true_pos+false_neg+false_pos\n",
    "\n",
    "print(\"Precision % = \" + str(true_pos*100/(true_pos+false_pos)))\n",
    "print(\"Recall % = \" + str(true_pos*100/(false_neg+true_pos)))\n",
    "print(\"False Positive % = \" + str(false_pos*100/total))\n",
    "print(\"False Negative % = \" + str(false_neg*100/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5592341223859184\n",
      "[[46467 29733]\n",
      " [34171 34613]]\n",
      "Precision % = 53.79199950268859\n",
      "Recall % = 50.3212956501512\n",
      "False Positive % = 20.507780168846217\n",
      "False Negative % = 23.56880759256194\n"
     ]
    }
   ],
   "source": [
    "# now do the logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, Y_train)\n",
    "print(lr.score(X_test, Y_test))\n",
    "\n",
    "Y_hat = lr.predict(X_test)\n",
    "confusion = confusion_matrix(Y_test, Y_hat)\n",
    "print(confusion)\n",
    "\n",
    "true_neg = float(confusion[0][0])\n",
    "true_pos = float(confusion[1][1])\n",
    "false_neg = float(confusion[1][0])\n",
    "false_pos = float(confusion[0][1])\n",
    "total = true_neg+true_pos+false_neg+false_pos\n",
    "\n",
    "print(\"Precision % = \" + str(true_pos*100/(true_pos+false_pos)))\n",
    "print(\"Recall % = \" + str(true_pos*100/(false_neg+true_pos)))\n",
    "print(\"False Positive % = \" + str(false_pos*100/total))\n",
    "print(\"False Negative % = \" + str(false_neg*100/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6045149809634166\n",
      "[[48666 27534]\n",
      " [29805 38979]]\n",
      "Precision % = 58.603581254792296\n",
      "Recall % = 56.668702023726446\n",
      "False Positive % = 18.99106108260222\n",
      "False Negative % = 20.557440821056115\n"
     ]
    }
   ],
   "source": [
    "# now do the knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3).fit(X_train, Y_train)\n",
    "print(knn.score(X_test, Y_test))\n",
    "\n",
    "Y_hat = knn.predict(X_test)\n",
    "confusion = confusion_matrix(Y_test, Y_hat)\n",
    "print(confusion)\n",
    "\n",
    "true_neg = float(confusion[0][0])\n",
    "true_pos = float(confusion[1][1])\n",
    "false_neg = float(confusion[1][0])\n",
    "false_pos = float(confusion[0][1])\n",
    "total = true_neg+true_pos+false_neg+false_pos\n",
    "\n",
    "print(\"Precision % = \" + str(true_pos*100/(true_pos+false_pos)))\n",
    "print(\"Recall % = \" + str(true_pos*100/(false_neg+true_pos)))\n",
    "print(\"False Positive % = \" + str(false_pos*100/total))\n",
    "print(\"False Negative % = \" + str(false_neg*100/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.551453953539701\n",
      "[[43997 32203]\n",
      " [32829 35955]]\n",
      "Precision % = 52.7524281815781\n",
      "Recall % = 52.27233077459874\n",
      "False Positive % = 22.211416432158032\n",
      "False Negative % = 22.643188213871877\n"
     ]
    }
   ],
   "source": [
    "# now do the svm\n",
    "from sklearn import svm\n",
    "\n",
    "s = svm.SVC(kernel='linear', gamma='auto').fit(X_train, Y_train)\n",
    "print(s.score(X_test, Y_test))\n",
    "\n",
    "Y_hat = s.predict(X_test)\n",
    "confusion = confusion_matrix(Y_test, Y_hat)\n",
    "print(confusion)\n",
    "\n",
    "true_neg = float(confusion[0][0])\n",
    "true_pos = float(confusion[1][1])\n",
    "false_neg = float(confusion[1][0])\n",
    "false_pos = float(confusion[0][1])\n",
    "total = true_neg+true_pos+false_neg+false_pos\n",
    "\n",
    "print(\"Precision % = \" + str(true_pos*100/(true_pos+false_pos)))\n",
    "print(\"Recall % = \" + str(true_pos*100/(false_neg+true_pos)))\n",
    "print(\"False Positive % = \" + str(false_pos*100/total))\n",
    "print(\"False Negative % = \" + str(false_neg*100/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
